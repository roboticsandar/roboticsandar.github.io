{"hash":"ad09b57c30394a519eea03c5f2a04ae8b3f6f46c","data":{"tag":{"title":"Augmented Reality","belongsTo":{"edges":[{"node":{"title":"Visual SLAM tutorials | All you need to know about visual slam","path":"/visual-slam-tutorials-all-you-need-to-know-about-visual-slam/","date":"22. October 2020","timeToRead":2,"description":"Visual SLAM has received much attention in the computer vision community in the last few years, as more challenging data sets become available and it's nature of requiring only camera , without depending  on external sensors. Visual SLAM is starting to be implemented on mobile cameras and used in AR and other applications. ","content":"<p>Visual SLAM has received much attention in the computer vision community in the last few years, as more challenging data sets become available and it's nature of requiring only camera , without depending  on external sensors.Visual SLAM is starting to be implemented on mobile cameras and used in AR and other applications</p>\n<p>Let's start with the SLAM ...</p>\n<h2 id=\"what-is-slam-\"><a href=\"#what-is-slam-\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>What is SLAM ?</h2>\n<p>SLAM can be classified into the following: </p>\n<h3 id=\"filter-based-approach\"><a href=\"#filter-based-approach\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Filter based Approach</h3>\n<p>The Filter-based approach which is the classical approach that performs  prediction and update steps recursively. It maintains the information about the environment and the states of the robot as a probability density function. This includes the Kalman filter family (EKF, UKF, SEIF, etc.) in addition to the particle filter as well. </p>\n<h3 id=\"global-optimization-graphslam-approach\"><a href=\"#global-optimization-graphslam-approach\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Global optimization (GraphSLAM) Approach</h3>\n<p>The global optimization (GraphSLAM) approach which is based on saving some keyframes in the environment and uses bundle adjustment to estimate the motion. This is currently a popular approach for vision-based SLAM such as ORB-SLAM which is mentioned earlier and also Google's cartographer. </p>\n<h3 id=\"deep-learning-approach\"><a href=\"#deep-learning-approach\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Deep Learning Approach</h3>\n<p>Convolutional neural networks SLAM which is currently available as RatSLAM and proved in some situations to have superior performance. </p>\n<h2 id=\"what-is-localization-\"><a href=\"#what-is-localization-\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>What is Localization ?</h2>\n<p>Localization answers the question “Where Iam ?” . In more detail form it estimates the pose of the robot or Agents . </p>\n<h2 id=\"research--writing-your-own-slam\"><a href=\"#research--writing-your-own-slam\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Research : Writing Your Own SLAM</h2>\n<p>Writing and using visual SLAM requires a large amount of professional knowledge, and the real-time nature of the algorithm does not meet the practical requirements.\nIf things were so simple, SLAM theory would not have been studied by so many people for more than 30 years (it has been studied since the 1990s).</p>\n<p>You can have a general understanding of this field. However, the more you understand, the more difficult this direction becomes.</p>\n<p>SLAM can be implemented in many ways. First of all, there is a huge amount of different hardware that can be used. Secondly, SLAM is more like a concept than a single algorithm. There are many steps involved in SLAM and these different steps can be implemented using a number of different algorithms</p>\n<h2 id=\"application-and-implementation\"><a href=\"#application-and-implementation\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Application and Implementation</h2>\n"}},{"node":{"title":"What’s New in ARKit 4?","path":"/what-s-new-in-ar-kit-4/","date":"19. October 2020","timeToRead":1,"description":"Apple announced ARKit 4 alongside iOS 14 and iPadOS 14. The new version of ARKit introduces Location Anchors, a new Depth API, and improved face tracking. ","content":"<p>Apple announced ARKit 4 alongside iOS 14 and iPadOS 14. The new version of ARKit introduces Location Anchors, a new Depth API, and improved face tracking.</p>\n<h2 id=\"location-anchors\"><a href=\"#location-anchors\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Location Anchors</h2>\n<p>Location Anchors allow developers to place AR experiences, such as life‑size art installations or navigational directions, at a fixed destination. Location Anchoring leverages the higher-resolution data in Apple Maps to place AR experiences at a particular point in the world, meaning AR experiences may now be placed at specific locations, such as throughout cities or alongside famous landmarks.</p>\n<p>Users can move around virtual objects and observe them from different perspectives, exactly as real objects are seen through a camera lens.</p>\n<h2 id=\"new-depth-api\"><a href=\"#new-depth-api\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>New Depth API</h2>\n<p>ARKit 4 also takes advantage of iPad Pro's LiDAR Scanner with a brand-new Depth API with advanced scene understanding capabilities, creating a new way to access detailed per-pixel depth information. When combined with 3D mesh data, this depth information makes virtual object occlusion more realistic by enabling instant placement of virtual objects within their physical surroundings. </p>\n<p>This can offer new capabilities for developers, such as taking more precise measurements and applying effects to the environment.</p>\n<h2 id=\"face-tracking\"><a href=\"#face-tracking\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Face Tracking</h2>\n<p>Finally, face tracking is expanded in ARKit 4 to support the front-facing camera on all devices with the A12 Bionic chip or newer. Up to three faces may now be tracked at once using the TrueDepth camera to power front-facing camera experiences like Memoji and Snapchat.</p>\n"}},{"node":{"title":"New virtual reality software allows scientists to ‘walk’ inside cells","path":"/new-virtual-reality-software-allows-scientists-to-walk-inside-cells/","date":"7. January 2020","timeToRead":1,"description":"vLUME is a virtual reality software package designed to render large three-dimensional single-molecule localization microscopy datasets.","content":"<p>vLUME is a virtual reality software package designed to render large three-dimensional single-molecule localization microscopy datasets. vLUME features include visualization, segmentation, bespoke analysis of complex local geometries and exporting features. vLUME can perform complex analysis on real three-dimensional biological samples that would otherwise be impossible by using regular flat-screen visualization programs.</p>\n<p>Nature(<a href=\"https://www.nature.com/articles/s41592-020-0962-1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.nature.com/articles/s41592-020-0962-1</a>) </p>\n"}},{"node":{"title":"The hierarchical localization toolbox","path":"/the-hierarchical-localization-toolbox/","date":"7. January 2019","timeToRead":1,"description":"This is hloc, a modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. ","content":"<p>This is hloc, a modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable. </p>\n<h2 id=\"general-pipeline\"><a href=\"#general-pipeline\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>General pipeline</h2>\n<p>The toolbox is composed of scripts, which roughly perform the following steps:</p>\n<ol>\n<li>Extract SuperPoint local features for all database and query images</li>\n<li>\n<p>Build a reference 3D SfM model</p>\n<ol>\n<li>Find covisible database images, with retrieval or a prior SfM model</li>\n<li>Match these database pairs with SuperGlue</li>\n<li>Triangulate a new SfM model with COLMAP</li>\n</ol>\n</li>\n<li>Find database images relevant to each query, using retrieval</li>\n<li>Match the query images with SuperGlue</li>\n<li>Run the localization</li>\n<li>Visualize and debug</li>\n</ol>\n<p>Github(<a href=\"https://github.com/cvg/Hierarchical-Localization\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://github.com/cvg/Hierarchical-Localization</a>) </p>\n"}}]}}},"context":{}}